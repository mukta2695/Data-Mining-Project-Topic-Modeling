# -*- coding: utf-8 -*-
"""Group 8 CS535-01 Data Mining Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QEdQJmGOeXjkd3qrErSz3f-rIkk-NGO7

# Topic Modeling

Team: Mukta Parab (mparab1@binghamton.edu)
Ami Chauhan(achauha4@binghamton.edu),
Jacob Ditkoff(jditkof1@binghamton.edu),
Meghana Harisha (mharish1@binghamton.edu),
Prarthna Mohanraj (pmohanr1@binghamton.edu)

**1. What is the problem the three papers aim to solve, and why is this problem important or interesting?**

Accurately modeling collections of discrete data through topics, while 
maintaining relationships between data so tasks like classification, novelty detection and summarization can be done on the data. 

Probabilistic topic models are important because they allow us to organize and quantify huge sums of data at a scale that would be impossible for humans to do alone. It can also be used to identify the topics of a set of collections by detecting the patterns and recurring words.

**2. 1) Summarize the three methods, including high-level ideas as well as technical details: the relevant details that are important to focus on (e.g., if there’s a model, define it; if there is a theorem, state it and explain why it’s important, etc) 2) What are the major differences of the three methods?**

**Paper 1**


*   LDA is a three level heirarchal Bayesian model: each item of a collection is modeled as an infinite mixture over an underlying set of topics. 
*   For each document (sequence of n words) w in a corpus (collection of M documents) D LDA assumes the following generative process:
        1. Choose N ∼ Poisson(ξ).
        2. Choose θ ∼ Dir(α).
        3. For each of the N words wn:
                (a) Choose a topic zn ∼ Multinomial(θ).
                (b) Choose a word wn from p(wn |zn,β), a multinomial probability conditioned on the topic zn.
*   The topic node is repeatedly sampled within the doument.
*   Documents can be associated with multiple topics.
*   Provides well defined inference procedures for previously unseen documents.
*   Modularity and extensibiltiy of this model is an advantage.

**Paper 2**
*   Autoencoded Variational Inference (AVITM) : a black box method that is easy to apply to new models. It yields topics of equivalent quality to standard mean field inference with a decrease in training time. It matches traditional methods and accuracy with much better inference time.
*   Model: PRODLDA
*   It is a product of experts, p(wn|θ, β) ∝ k p(wn|zn = k, β) θk 
*   Defined as latent Dirichlet allocation where the word-level mixture over topics is carried out in natural parameter space. 
*   The difference between LDA and PRODLDA is that the beta is unnormalized.
*   Yields much more interpretable topics.
*   PRODLDA is an instance of the exponential-family PCA.
*   It relates to the exponential-family harmoniums but with nonGaussian priors.

**Paper 3**

  *   top2vec leverages joint document and word semantic embedding to find the topic vectors
  *   This model automatically finds the number of topics and does not need stop-word lists or stemming and lemmatization.
  *   The main assumption for this model is that the number of prominent topics is equal to the number of dense areas.
  *   The top2vec model produces jointly embedded topics, documents, and word vectors. The distance between them represents semantic similarity.
  *   top2vec model finds topics that are significantly more informative and representative of the corpus trained on compared to the 
probabilistic generative models.

**3. Implement the ProdLDA [2] topic model and test it on the 20 newsgroups text dataset (report the 20 main topics you discovered from the data and visualize their word clouds)**
"""

!pip install pyro-ppl

import os
import pyro
import pyro.distributions as dist
from pyro.infer import MCMC, NUTS
import torch

assert pyro.__version__.startswith('1.7.0')
smoke_test = 'CI' in os.environ

import pandas as pd
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer

news = fetch_20newsgroups(subset='all')
vectorizer = CountVectorizer(max_df=0.5, min_df=20, stop_words='english')
docs = torch.from_numpy(vectorizer.fit_transform(news['data']).toarray())

vocab = pd.DataFrame(columns=['word', 'index'])
vocab['word'] = vectorizer.get_feature_names()
vocab['index'] = vocab.index

import math
import torch.nn as nn
import torch.nn.functional as F
from pyro.infer import SVI, TraceMeanField_ELBO
from tqdm import trange

class Encoder(nn.Module):
    def __init__(self, vocab_size, num_topics, hidden, dropout):
        super().__init__()
        self.drop = nn.Dropout(dropout)  
        self.fc1 = nn.Linear(vocab_size, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.fcmu = nn.Linear(hidden, num_topics)
        self.fclv = nn.Linear(hidden, num_topics)
        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  
        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  

    def forward(self, inputs):
        h = F.softplus(self.fc1(inputs))
        h = F.softplus(self.fc2(h))
        h = self.drop(h)

        logtheta_loc = self.bnmu(self.fcmu(h))
        logtheta_logvar = self.bnlv(self.fclv(h))
        logtheta_scale = (0.5 * logtheta_logvar).exp()  
        return logtheta_loc, logtheta_scale


class Decoder(nn.Module):
    def __init__(self, vocab_size, num_topics, dropout):
        super().__init__()
        self.beta = nn.Linear(num_topics, vocab_size, bias=False)
        self.bn = nn.BatchNorm1d(vocab_size, affine=False)
        self.drop = nn.Dropout(dropout)

    def forward(self, inputs):
        inputs = self.drop(inputs)
        return F.softmax(self.bn(self.beta(inputs)), dim=1)


class ProdLDA(nn.Module):
    def __init__(self, vocab_size, num_topics, hidden, dropout):
        super().__init__()
        self.vocab_size = vocab_size
        self.num_topics = num_topics
        self.encoder = Encoder(vocab_size, num_topics, hidden, dropout)
        self.decoder = Decoder(vocab_size, num_topics, dropout)

    def model(self, docs):
        pyro.module("decoder", self.decoder)
        with pyro.plate("documents", docs.shape[0]):
            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics))
            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics))
            logtheta = pyro.sample(
                "logtheta", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))
            theta = F.softmax(logtheta, -1)


            count_param = self.decoder(theta)
            total_count = int(docs.sum(-1).max())
            pyro.sample(
                'obs',
                dist.Multinomial(total_count, count_param),
                obs=docs
            )

    def guide(self, docs):
        pyro.module("encoder", self.encoder)
        with pyro.plate("documents", docs.shape[0]):
            logtheta_loc, logtheta_scale = self.encoder(docs)
            logtheta = pyro.sample(
                "logtheta", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))

    def beta(self):
        return self.decoder.beta.weight.cpu().detach().T

# setting global variables
seed = 0
torch.manual_seed(seed)
pyro.set_rng_seed(seed)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

num_topics = 20 if not smoke_test else 3
docs = docs.float().to(device)
batch_size = 32
learning_rate = 1e-3
num_epochs = 50 if not smoke_test else 1

# training
pyro.clear_param_store()

prodLDA = ProdLDA(
    vocab_size=docs.shape[1],
    num_topics=num_topics,
    hidden=100 if not smoke_test else 10,
    dropout=0.2
)
prodLDA.to(device)

optimizer = pyro.optim.Adam({"lr": learning_rate})
svi = SVI(prodLDA.model, prodLDA.guide, optimizer, loss=TraceMeanField_ELBO())
num_batches = int(math.ceil(docs.shape[0] / batch_size)) if not smoke_test else 1

bar = trange(num_epochs)
for epoch in bar:
    running_loss = 0.0
    for i in range(num_batches):
        batch_docs = docs[i * batch_size:(i + 1) * batch_size, :]
        loss = svi.step(batch_docs)
        running_loss += loss / batch_docs.size(0)

    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))

import matplotlib.pyplot as plt
from wordcloud import WordCloud
def plot_word_cloud(b, ax, v, n):
    sorted_, indices = torch.sort(b, descending=True)
    df = pd.DataFrame(indices[:100].numpy(), columns=['index'])
    words = pd.merge(df, vocab[['index', 'word']],
                     how='left', on='index')['word'].values.tolist()
    sizes = (sorted_[:100] * 1000).float().numpy().tolist()
    freqs = {words[i]: sizes[i] for i in range(len(words))}
    wc = WordCloud(background_color="white", width=800, height=500)
    wc = wc.generate_from_frequencies(freqs)
    ax.set_title('Topic %d' % (n + 1))
    ax.imshow(wc, interpolation='bilinear')
    ax.axis("off")

if not smoke_test:
    
    beta = prodLDA.beta()
    fig, axs = plt.subplots(7, 3, figsize=(14, 24))
    for n in range(beta.shape[0]):
        i, j = divmod(n, 3)
        plot_word_cloud(beta[n], axs[i, j], vocab, n)
    axs[-1, -1].axis('off');

    plt.show()

"""**4. Implement Latent Dirichlet Allocation (LDA) [1] and test it on the 20 newsgroups text dataset (report the 20 main topics you discovered from the data and visualize their word clouds)**"""

#Imports and getting the data

!pip install gensim
!pip install nltk

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
import nltk
import nltk.stem as stemmer
from nltk.stem import *
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
nltk.download('wordnet')
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(400)
import pandas as pd 
from wordcloud import WordCloud
from sklearn.datasets import fetch_20newsgroups
newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)
newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)

#Preprocessing the data by tokenizing and lemmatizing it

def lemmatize_stemming(text):
    stemmer = PorterStemmer()
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
# Tokenize and lemmatize
def preprocess(text):
    result=[]
    for token in gensim.utils.simple_preprocess(text) :
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
            
    return result

#Putting data into 'bag of words'

processed_docs = []

for doc in newsgroups_train.data:
    processed_docs.append(preprocess(doc))

dictionary = gensim.corpora.Dictionary(processed_docs)
bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

#Running the model

lda_model =  gensim.models.LdaMulticore(bow_corpus, num_topics = 20, id2word = dictionary, passes = 10, workers = 2)

#Generating the wordcloud

for topic in range(lda_model.num_topics):
    #print(lda_model.num_topics)
    plt.figure()
    plt.imshow(WordCloud().fit_words(dict(lda_model.show_topic(topic, 200))))
    plt.axis("off")
    plt.title("Topic #" + str(topic))
    plt.show()

"""**5. Implement TOP2VEC [3] and test it on the 20 newsgroups text dataset (report the 20 main topics you discovered from the data and visualize their word clouds)**

"""

!pip install top2vec

from top2vec import Top2Vec
from sklearn.datasets import fetch_20newsgroups

newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
model = Top2Vec(documents=newsgroups.data, speed="learn", workers=8)

topic_words, word_scores, topic_scores = model.get_topics()
topic_list = []
for doc in topic_words:
  for topic in doc:
    topic_list.append(topic)

topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=topic_list, num_topics=20)
for topic in topic_nums:
  model.generate_topic_wordcloud(topic)

"""**6. Compare the experimental results of the three methods with the ground truth topics of the 20 newsgroups. Which method do you think is better? Explain why.**

- All three models produced different results when visualized through wordclouds.
- ProdLDA took the longest to train using the 20newsgroups dataset.
- Top2Vec was the quickest to train with 20newsgroups.
- Top2Vec seemed to produce topics that were more representative of the data it was trained on compared to LDA and ProdLDA.

**7. Fetch real-time tweets and discover real-time topics on them with the three methods, respectively. You need to report the top 5 topics for the last 5 seconds in real-time and the top 5 topics for each hour in the most recent 24 hours.**
"""

#Printing twitter topics for the last 5 seconds

!pip install torch
!pip install requests
!pip install pytz
!pip install top2vec
!pip install gensim
!pip install nltk
!pip install top2vec

from wordcloud import WordCloud
import nltk
import nltk.stem as stemmer
from nltk.stem import *
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
nltk.download('wordnet')
import pandas as pd
import numpy as np
import torch
from sklearn.feature_extraction.text import CountVectorizer
import requests
import os
import json
from pytz import timezone
from top2vec import Top2Vec
import gensim
from datetime import datetime, timedelta



# These two functions are for LDA
def lemmatize_stemming(text):
    stemmer = PorterStemmer()
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
# Tokenize and lemmatize
def preprocess(text):
    result=[]
    for token in gensim.utils.simple_preprocess(text) :
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
            
    return result
# End of LDA functions



# This is a throwaway token for a twitter dev account we made. Best practice is to export this token to an OS environment variable and use that instead so no tokens are outward facing
bearer_token = "AAAAAAAAAAAAAAAAAAAAANpDWgEAAAAAXP3yKjur0GnXLyAyICG%2BdRmo03w%3D271I8otWNMlgqYMsm3vXHHOUX1TovJgE53IiAqqZqC7iMPe0CA"

#Creating headers and formatting our request to Twitter API endpoint
headers = {"Authorization": "Bearer {}".format(bearer_token)}
search_url = "https://api.twitter.com/2/tweets/search/recent"
eastern = timezone('US/Eastern')
current_time = datetime.now(eastern)
start_time = current_time - timedelta(0,5)
start_time = start_time.isoformat()[:-8] + "Z"
end_time = current_time.isoformat()[:-8] + "Z"

query_params = {'query': "all lang:en",
                    'start_time': start_time,
                    'end_time': end_time,
                    'max_results': 100,
                    'tweet.fields': 'id,text'
                }

response = requests.request("GET", search_url, headers = headers, params = query_params)
if response.status_code != 200:
  raise Exception(response.status_code, response.text)
twitter_response = response.json()
data = twitter_response['data']

#Once we have our data, we extract the tweets from it so we can run the models
tweets = []
for item in data:
  tweets.append(item['text'])


print(
    '''
    ------------------------------------------------
     Printing Twitter topics for the past 5 seconds
    ------------------------------------------------
    '''

)

#LDA CODE

print(
'''
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
------------------------------------------------------------------------------------
                                    LDA Model

'''
)

processed_docs = []

for tweet in tweets:
   processed_docs.append(preprocess(tweet))

dictionary = gensim.corpora.Dictionary(processed_docs)
bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

lda_model =  gensim.models.LdaMulticore(bow_corpus, num_topics = 5, id2word = dictionary, passes = 10, workers = 2)
for topic in range(lda_model.num_topics):
    print("Topic:")
    print(lda_model.show_topic(topic, 5))

# END OF LDA CODE


# ------------------------------------------------------------------------------------ #


#TOP2VEC CODE

proc_docs = []

for tweet in tweets:
   proc_docs.append(preprocess(tweet))

flatList = [item for elem in proc_docs for item in elem]
print(
 '''
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
                                     Top2Vec Model

 '''
 )

top2vec_model = Top2Vec(documents=flatList, speed="learn", workers=10,min_count=1)
print("Topics Found:")
print(top2vec_model.get_num_topics())
print("Topics:")
print(top2vec_model.topic_words)


#END TOP2VEC CODE

!pip install pyro-ppl

import gensim
from datetime import datetime, timedelta
import pyro
import pyro.distributions as dist
from pyro.infer import MCMC, NUTS
import torch

import math
import torch.nn as nn
import torch.nn.functional as F
from pyro.infer import SVI, TraceMeanField_ELBO
from tqdm import trange

assert pyro.__version__.startswith('1.7.0')
#Enable smoke test - run the notebook cells on CI.
smoke_test = 'CI' in os.environ

import pandas as pd
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer

print(
 '''
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
 ------------------------------------------------------------------------------------
                                     ProdLDA Model

 '''
 )
class Encoder(nn.Module):
    # Base class for the encoder net, used in the guide
    def __init__(self, vocab_size, num_topics, hidden, dropout):
        super().__init__()
        self.drop = nn.Dropout(dropout)  # to avoid component collapse
        self.fc1 = nn.Linear(vocab_size, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.fcmu = nn.Linear(hidden, num_topics)
        self.fclv = nn.Linear(hidden, num_topics)
        # NB: here we set `affine=False` to reduce the number of learning parameters
        # See https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html
        # for the effect of this flag in BatchNorm1d
        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse
        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse

    def forward(self, inputs):
        h = F.softplus(self.fc1(inputs))
        h = F.softplus(self.fc2(h))
        h = self.drop(h)
        # μ and Σ are the outputs
        logtheta_loc = self.bnmu(self.fcmu(h))
        logtheta_logvar = self.bnlv(self.fclv(h))
        logtheta_scale = (0.5 * logtheta_logvar).exp()  # Enforces positivity
        return logtheta_loc, logtheta_scale


class Decoder(nn.Module):
    # Base class for the decoder net, used in the model
    def __init__(self, vocab_size, num_topics, dropout):
        super().__init__()
        self.beta = nn.Linear(num_topics, vocab_size, bias=False)
        self.bn = nn.BatchNorm1d(vocab_size, affine=False)
        self.drop = nn.Dropout(dropout)

    def forward(self, inputs):
        inputs = self.drop(inputs)
        # the output is σ(βθ)
        return F.softmax(self.bn(self.beta(inputs)), dim=1)


class ProdLDA(nn.Module):
    def __init__(self, vocab_size, num_topics, hidden, dropout):
        super().__init__()
        self.vocab_size = vocab_size
        self.num_topics = num_topics
        self.encoder = Encoder(vocab_size, num_topics, hidden, dropout)
        self.decoder = Decoder(vocab_size, num_topics, dropout)

    def model(self, docs):
        pyro.module("decoder", self.decoder)
        with pyro.plate("documents", docs.shape[0]):
            # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution
            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics))
            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics))
            logtheta = pyro.sample(
                "logtheta", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))
            theta = F.softmax(logtheta, -1)

            # conditional distribution of 𝑤𝑛 is defined as
            # 𝑤𝑛|𝛽,𝜃 ~ Categorical(𝜎(𝛽𝜃))
            count_param = self.decoder(theta)
            # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.
            # Because the numbers of words across documents can vary,
            # we will use the maximum count accross documents here.
            # This does not affect the result because Multinomial.log_prob does
            # not require `total_count` to evaluate the log probability.
            total_count = int(docs.sum(-1).max())
            pyro.sample(
                'obs',
                dist.Multinomial(total_count, count_param),
                obs=docs
            )

    def guide(self, docs):
        pyro.module("encoder", self.encoder)
        with pyro.plate("documents", docs.shape[0]):
            # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution,
            # where μ and Σ are the encoder network outputs
            logtheta_loc, logtheta_scale = self.encoder(docs)
            logtheta = pyro.sample(
                "logtheta", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))

    def beta(self):
        # beta matrix elements are the weights of the FC layer on the decoder
        return self.decoder.beta.weight.cpu().detach().T


# setting global variables

# This is a throwaway token for a twitter dev account we made. Best practice is to export this token to an OS environment variable and use that instead so no tokens are outward facing
bearer_token = "AAAAAAAAAAAAAAAAAAAAANpDWgEAAAAAXP3yKjur0GnXLyAyICG%2BdRmo03w%3D271I8otWNMlgqYMsm3vXHHOUX1TovJgE53IiAqqZqC7iMPe0CA"

#Creating headers and formatting our request to Twitter API endpoint
headers = {"Authorization": "Bearer {}".format(bearer_token)}
search_url = "https://api.twitter.com/2/tweets/search/recent"
eastern = timezone('US/Eastern')
current_time = datetime.now(eastern)
start_time = current_time - timedelta(0,5)
start_time = start_time.isoformat()[:-8] + "Z"
end_time = current_time.isoformat()[:-8] + "Z"

query_params = {'query': "all lang:en",
                        'start_time': start_time,
                        'end_time': end_time,
                        'max_results': 100,
                        'tweet.fields': 'id,text'
                }

response = requests.request("GET", search_url, headers = headers, params = query_params)
if response.status_code != 200:
  raise Exception(response.status_code, response.text)
twitter_response = response.json()
data = twitter_response['data']

#Once we have our data, we extract the tweets from it so we can run the models
tweets = []
for item in data:
  tweets.append(item['text'])

          # END OF GRABBING TWEETS #

vectorizer = CountVectorizer(ngram_range=(1,1), min_df = 0.5)
docs = torch.from_numpy(vectorizer.fit_transform(tweets).toarray())

vocab = pd.DataFrame(columns=['word', 'index'])
vocab['word'] = vectorizer.get_feature_names()
vocab['index'] = vocab.index
        
seed = 0
torch.manual_seed(seed)
pyro.set_rng_seed(seed)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

num_topics = 5 if not smoke_test else 3
docs = docs.float().to(device)
batch_size = 32
learning_rate = 1e-3
num_epochs = 50 if not smoke_test else 1

        # training

pyro.clear_param_store()

prodLDA = ProdLDA(
        vocab_size=docs.shape[1],
        num_topics=num_topics,
        hidden=1 if not smoke_test else 10,
        dropout=0.2
    )
prodLDA.to(device)

optimizer = pyro.optim.Adam({"lr": learning_rate})
svi = SVI(prodLDA.model, prodLDA.guide, optimizer, loss=TraceMeanField_ELBO())
num_batches = int(math.ceil(docs.shape[0] / batch_size)) if not smoke_test else 1

bar = trange(num_epochs)
for epoch in bar:
    running_loss = 0.0
    for i in range(num_batches):
        batch_docs = docs[i * batch_size:(i + 1) * batch_size, :]
        loss = svi.step(batch_docs)
        running_loss += loss / batch_docs.size(0)

    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))

print("ProdLDA Topics for the last 5 seconds: ")
print(vocab['word'])

#Printing top Twitter Topics of every hour for the past 24 hours
def LDA(x):

        #Creating headers and formatting our request to Twitter API endpoint
        headers = {"Authorization": "Bearer {}".format(bearer_token)}
        search_url = "https://api.twitter.com/2/tweets/search/recent"
        eastern = timezone('US/Eastern')
        current_time = datetime.now(eastern)
        start_time = current_time - timedelta(0,x)
        start_time = start_time.isoformat()[:-8] + "Z"
        end_time = current_time.isoformat()[:-8] + "Z"

        query_params = {'query': "all lang:en",
                            'start_time': start_time,
                            'end_time': end_time,
                            'max_results': 100,
                            'tweet.fields': 'id,text'
                        }

        response = requests.request("GET", search_url, headers = headers, params = query_params)
        if response.status_code != 200:
          raise Exception(response.status_code, response.text)
        twitter_response = response.json()
        data = twitter_response['data']

        #Once we have our data, we extract the tweets from it so we can run the models
        tweets = []
        for item in data:
          tweets.append(item['text'])


        print(
            '''
            -----------------------------------------------------------------
                                Printing Twitter topics
            -----------------------------------------------------------------
            '''

        )

      #LDA CODE

        print(
            '''
            ------------------------------------------------------------------------------------
                                                LDA Model
            ------------------------------------------------------------------------------------
            '''
            )


        processed_docs = []
        for tweet in tweets:
           processed_docs.append(preprocess(tweet))
        dictionary = gensim.corpora.Dictionary(processed_docs)
        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

        lda_model =  gensim.models.LdaMulticore(bow_corpus, num_topics = 5, id2word = dictionary, passes = 10, workers = 2)
        for topic in range(lda_model.num_topics):
            print("Topic:")
            print(lda_model.show_topic(topic, 5))

        # END OF LDA CODE
        # ------------------------------------------------------------------------------------ #

def hourly_LDA():
  x=3600;
  while (x <= 86400):
      LDA(x)
      x= x+3600

hourly_LDA()

def TOP(x):

        #Creating headers and formatting our request to Twitter API endpoint
        headers = {"Authorization": "Bearer {}".format(bearer_token)}
        search_url = "https://api.twitter.com/2/tweets/search/recent"
        eastern = timezone('US/Eastern')
        current_time = datetime.now(eastern)
        start_time = current_time - timedelta(0,x)
        start_time = start_time.isoformat()[:-8] + "Z"
        end_time = current_time.isoformat()[:-8] + "Z"

        query_params = {'query': "all lang:en",
                            'start_time': start_time,
                            'end_time': end_time,
                            'max_results': 100,
                            'tweet.fields': 'id,text'
                        }

          #TOP2VEC CODE    
        proc_docs = []

        for tweet in tweets:
              proc_docs.append(preprocess(tweet))

        flatList = [item for elem in proc_docs for item in elem]
        print(
            '''
            ------------------------------------------------------------------------------------
                                                Top2Vec Model
            ------------------------------------------------------------------------------------
            '''
            )

        top2vec_model = Top2Vec(documents=flatList, speed="learn", workers=10,min_count=1)
        print("Topics Found:")
        print(top2vec_model.get_num_topics())
        print("Topics:")
        print(top2vec_model.topic_words)


          #END TOP2VEC CODE

def hourly_Top2Vec():
  x=3600;
  while (x <= 86400):
      TOP(x)
      x= x+3600

hourly_Top2Vec()

def PROD(x):
    print(
    '''
    ------------------------------------------------------------------------------------
    ------------------------------------------------------------------------------------
    ------------------------------------------------------------------------------------
    ------------------------------------------------------------------------------------
    ------------------------------------------------------------------------------------
    ------------------------------------------------------------------------------------
                                        ProdLDA Model

    '''
    )
    class Encoder(nn.Module):
        # Base class for the encoder net, used in the guide
        def __init__(self, vocab_size, num_topics, hidden, dropout):
            super().__init__()
            self.drop = nn.Dropout(dropout)  # to avoid component collapse
            self.fc1 = nn.Linear(vocab_size, hidden)
            self.fc2 = nn.Linear(hidden, hidden)
            self.fcmu = nn.Linear(hidden, num_topics)
            self.fclv = nn.Linear(hidden, num_topics)
            self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse
            self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse

        def forward(self, inputs):
            h = F.softplus(self.fc1(inputs))
            h = F.softplus(self.fc2(h))
            h = self.drop(h)
            # μ and Σ are the outputs
            logtheta_loc = self.bnmu(self.fcmu(h))
            logtheta_logvar = self.bnlv(self.fclv(h))
            logtheta_scale = (0.5 * logtheta_logvar).exp()  # Enforces positivity
            return logtheta_loc, logtheta_scale


    class Decoder(nn.Module):
        # Base class for the decoder net, used in the model
        def __init__(self, vocab_size, num_topics, dropout):
            super().__init__()
            self.beta = nn.Linear(num_topics, vocab_size, bias=False)
            self.bn = nn.BatchNorm1d(vocab_size, affine=False)
            self.drop = nn.Dropout(dropout)

        def forward(self, inputs):
            inputs = self.drop(inputs)
            # the output is σ(βθ)
            return F.softmax(self.bn(self.beta(inputs)), dim=1)


    class ProdLDA(nn.Module):
        def __init__(self, vocab_size, num_topics, hidden, dropout):
            super().__init__()
            self.vocab_size = vocab_size
            self.num_topics = num_topics
            self.encoder = Encoder(vocab_size, num_topics, hidden, dropout)
            self.decoder = Decoder(vocab_size, num_topics, dropout)

        def model(self, docs):
            pyro.module("decoder", self.decoder)
            with pyro.plate("documents", docs.shape[0]):
                # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution
                logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics))
                logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics))
                logtheta = pyro.sample(
                    "logtheta", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))
                theta = F.softmax(logtheta, -1)

                # conditional distribution of 𝑤𝑛 is defined as
                # 𝑤𝑛|𝛽,𝜃 ~ Categorical(𝜎(𝛽𝜃))
                count_param = self.decoder(theta)
                # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.
                # Because the numbers of words across documents can vary,
                # we will use the maximum count accross documents here.
                # This does not affect the result because Multinomial.log_prob does
                # not require `total_count` to evaluate the log probability.
                total_count = int(docs.sum(-1).max())
                pyro.sample(
                    'obs',
                    dist.Multinomial(total_count, count_param),
                    obs=docs
                )

        def guide(self, docs):
            pyro.module("encoder", self.encoder)
            with pyro.plate("documents", docs.shape[0]):
                # Dirichlet prior 𝑝(𝜃|𝛼) is replaced by a logistic-normal distribution,
                # where μ and Σ are the encoder network outputs
                logtheta_loc, logtheta_scale = self.encoder(docs)
                logtheta = pyro.sample(
                    "logtheta", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))

        def beta(self):
            # beta matrix elements are the weights of the FC layer on the decoder
            return self.decoder.beta.weight.cpu().detach().T


    # setting global variables

    # This is a throwaway token for a twitter dev account we made. Best practice is to export this token to an OS environment variable and use that instead so no tokens are outward facing
    bearer_token = "AAAAAAAAAAAAAAAAAAAAANpDWgEAAAAAXP3yKjur0GnXLyAyICG%2BdRmo03w%3D271I8otWNMlgqYMsm3vXHHOUX1TovJgE53IiAqqZqC7iMPe0CA"

    #Creating headers and formatting our request to Twitter API endpoint
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    search_url = "https://api.twitter.com/2/tweets/search/recent"
    eastern = timezone('US/Eastern')
    current_time = datetime.now(eastern)
    start_time = current_time - timedelta(0,x)
    start_time = start_time.isoformat()[:-8] + "Z"
    end_time = current_time.isoformat()[:-8] + "Z"

    query_params = {'query': "all lang:en",
                            'start_time': start_time,
                            'end_time': end_time,
                            'max_results': 100,
                            'tweet.fields': 'id,text'
                    }

    response = requests.request("GET", search_url, headers = headers, params = query_params)
    if response.status_code != 200:
      raise Exception(response.status_code, response.text)
    twitter_response = response.json()
    data = twitter_response['data']

    #Once we have our data, we extract the tweets from it so we can run the models
    tweets = []
    for item in data:
      tweets.append(item['text'])

              # END OF GRABBING TWEETS #

    vectorizer = CountVectorizer(ngram_range=(1,1), min_df = 0.5)
    docs = torch.from_numpy(vectorizer.fit_transform(tweets).toarray())

    vocab = pd.DataFrame(columns=['word', 'index'])
    vocab['word'] = vectorizer.get_feature_names()
    vocab['index'] = vocab.index
            
    seed = 0
    torch.manual_seed(seed)
    pyro.set_rng_seed(seed)
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    num_topics = 5 if not smoke_test else 3
    docs = docs.float().to(device)
    batch_size = 32
    learning_rate = 1e-3
    num_epochs = 50 if not smoke_test else 1

            # training

    pyro.clear_param_store()

    prodLDA = ProdLDA(
            vocab_size=docs.shape[1],
            num_topics=num_topics,
            hidden=1 if not smoke_test else 10,
            dropout=0.2
        )
    prodLDA.to(device)

    optimizer = pyro.optim.Adam({"lr": learning_rate})
    svi = SVI(prodLDA.model, prodLDA.guide, optimizer, loss=TraceMeanField_ELBO())
    num_batches = int(math.ceil(docs.shape[0] / batch_size)) if not smoke_test else 1

    bar = trange(num_epochs)
    for epoch in bar:
        running_loss = 0.0
        for i in range(num_batches):
            batch_docs = docs[i * batch_size:(i + 1) * batch_size, :]
            loss = svi.step(batch_docs)
            running_loss += loss / batch_docs.size(0)

        bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))

    print("ProdLDA Topics for the last 5 seconds: ")
    print(vocab)

def hourly_PRODLDA():
  x=3600;
  while (x <= 86400):
      PROD(x)
      x= x+3600

hourly_PRODLDA()

"""**8. Fetch real-time news and discover real-time topics on them with the three methods, respectively. You need to report the top 5 topics for each day in the most recent week.**"""

def daily_LDA():
  x=86400
  while (x <= 518400):
      LDA(x)
      x= x+86400

daily_LDA()

def daily_Top2Vec():
  x=86400
  while (x <= 518400):
      TOP(x)
      x= x+86400

daily_Top2Vec()

def daily_PRODLDA():
  x=86400
  while (x <= 518400):
      PROD(x)
      x= x+86400

daily_PRODLDA()